# Compose Composer Extension
# Dynamically assembles docker-compose files using the `include` directive
# to preserve relative path resolution for each component.
#
# Design Principle:
#   Any plugin can be the orchestrator (composer). The result should be the same
#   regardless of which plugin initiates the composition. This is achieved through
#   declarative wiring rules defined in extensions, not imperative callbacks.
#
# Usage:
#   load('ext://compose_composer', 'generate_master_compose', 'parse_cli_plugins')
#   
#   cli_plugins = parse_cli_plugins(os.path.dirname(__file__))
#   dependencies = core_dependencies + cli_plugins
#   master_compose = generate_master_compose(dependencies)
#   docker_compose(encode_yaml(master_compose))
#
# CLI Usage:
#   tilt up -- plugin-name ../relative/path /absolute/path https://github.com/org/repo.git
#
# Dependency schema:
#   name: str                      - Extension name (required)
#   url: str                       - Extension repo URL, file:// or https:// (required)
#   ref: str                       - Git ref for https:// URLs (default: 'main')
#   repo_path: str                 - Path within repo (default: name)
#   get_compose_path_function: str - Function to call for compose path (default: 'get_compose_path')
#   compose_overrides: dict        - Static overrides merged into compose (optional)
#
# Extension exports:
#   get_compose_path() -> str      - Returns path to compose file (required)
#   get_wire_when() -> dict        - Conditional wiring rules (optional)
#   get_provides() -> dict         - What this extension provides (optional, for docs)

# ============================================================================
# CLI Argument Parsing
# ============================================================================

config.define_string_list('args', args=True, usage='Plugins to load (name, path, or URL)')
_cfg = config.parse()

def _is_url(s):
    """Check if string is a URL (git or https)."""
    return '://' in s or s.startswith('git@')

def _resolve_plugin_spec(plugin, tiltfile_dir):
    """
    Resolve a plugin specifier to a dependency dict with url.
    - URLs pass through as https
    - Absolute paths become file:// URLs
    - Relative paths resolve from tiltfile_dir and become file:// URLs
    - Plain names are treated as adjacent directories
    """
    # Git/HTTPS URLs
    if _is_url(plugin):
        # Extract name from URL (last segment, minus extensions)
        name = plugin.split('/')[-1].split('#')[0].replace('.git', '')
        return {'name': name, 'url': plugin, '_from_cli': True}
    
    # Resolve local paths to absolute
    if plugin.startswith('/'):
        path = plugin
    elif plugin.startswith('.'):
        path = os.path.abspath(tiltfile_dir + '/' + plugin)
    else:
        # Adjacent directory - look in parent
        path = os.path.abspath(tiltfile_dir + '/../' + plugin)
    
    # Derive name from path
    name = os.path.basename(path)
    
    # Convert to file:// URL for extension loading
    # For file:// URLs, the path IS the repo, so repo_path should be '.'
    return {
        'name': name,
        'url': 'file://' + path,
        'repo_path': '.',
        '_from_cli': True,
    }

def parse_cli_plugins(tiltfile_dir):
    """
    Parse CLI positional args and return list of dependency dicts.
    
    Args:
        tiltfile_dir: Directory of the calling Tiltfile (for relative path resolution)
                      Typically: os.path.dirname(__file__)
    
    Returns:
        List of dependency dicts with 'name' and 'url' keys
    """
    plugins = _cfg.get('args', [])
    deps = []
    
    for plugin in plugins:
        dep = _resolve_plugin_spec(plugin, tiltfile_dir)
        deps.append(dep)
        print("CLI plugin: " + dep['name'] + " -> " + dep['url'])
    
    return deps

# ============================================================================
# Extension Loading
# ============================================================================

def _load_extension(dep):
    """Load a Tiltfile extension and populate its symbols."""
    name = dep['name']
    url = dep['url']
    ref = dep.get('ref', 'main')
    repo_path = dep.get('repo_path', name)
    
    if url.startswith('file://'):
        v1alpha1.extension_repo(name=name, url=url)
    else:
        v1alpha1.extension_repo(name=name, url=url, ref=ref)
    
    v1alpha1.extension(name=name, repo_name=name, repo_path=repo_path)
    dep['symbols'] = load_dynamic('ext://' + name)
    return dep

def _get_compose_path_from_extension(dep):
    """Get compose path from a loaded extension's function."""
    fn_name = dep.get('get_compose_path_function', 'get_compose_path')
    symbols = dep.get('symbols')
    
    if not symbols:
        fail("Extension " + dep['name'] + " has no symbols")
    
    fn = symbols.get(fn_name)
    if not fn:
        fail("Extension " + dep['name'] + " has no " + fn_name + " function")
    
    return fn()

# ============================================================================
# Deep Merge Utility
# ============================================================================

def _deep_merge(base, override):
    """
    Deep merge override into base.
    - Dicts are merged recursively
    - Lists are concatenated (for volumes, depends_on, etc.)
    - Other values are replaced
    
    Returns a new dict (does not modify base).
    """
    if type(base) != 'dict' or type(override) != 'dict':
        return override
    
    result = {}
    
    # Start with all keys from base
    for key in base:
        result[key] = base[key]
    
    # Merge/override with keys from override
    for key in override:
        if key in result:
            if type(result[key]) == 'dict' and type(override[key]) == 'dict':
                result[key] = _deep_merge(result[key], override[key])
            elif type(result[key]) == 'list' and type(override[key]) == 'list':
                # Concatenate lists, avoiding duplicates for simple values
                merged = list(result[key])
                for item in override[key]:
                    if item not in merged:
                        merged.append(item)
                result[key] = merged
            else:
                result[key] = override[key]
        else:
            result[key] = override[key]
    
    return result

def _deep_copy(obj):
    """Create a deep copy of a dict/list structure using YAML round-trip."""
    return decode_yaml(encode_yaml(obj))

# ============================================================================
# Declarative Wiring (wire_when)
# ============================================================================

def _collect_wire_when_rules(loaded_deps):
    """
    Collect wire_when rules from all loaded extensions.
    
    Returns a dict: {trigger_dep_name: [{source_dep, rules}, ...]}
    """
    all_rules = {}
    
    for dep in loaded_deps:
        symbols = dep.get('symbols', {})
        get_wire_when = symbols.get('get_wire_when')
        
        if not get_wire_when:
            continue
        
        wire_when = get_wire_when()
        
        for trigger_dep_name, rules in wire_when.items():
            if trigger_dep_name not in all_rules:
                all_rules[trigger_dep_name] = []
            
            all_rules[trigger_dep_name].append({
                'source_dep': dep['name'],
                'rules': rules,
            })
    
    return all_rules

def _apply_wire_when_rules(compose_yaml, dep_name, wire_when_rules, loaded_dep_names):
    """
    Apply wire_when rules to a compose file.
    
    Args:
        compose_yaml: The compose content to modify
        dep_name: Name of the dependency being processed
        wire_when_rules: All collected wire_when rules
        loaded_dep_names: List of all loaded dependency names
    
    Returns:
        Modified compose_yaml
    """
    modified = False
    
    # Check each trigger dependency
    for trigger_dep, rule_sets in wire_when_rules.items():
        # Only apply if the trigger dependency is loaded
        if trigger_dep not in loaded_dep_names:
            continue
        
        # Apply each rule set from extensions that defined rules for this trigger
        for rule_set in rule_sets:
            rules = rule_set['rules']
            source = rule_set['source_dep']
            
            # Get service rules
            service_rules = rules.get('services', {})
            
            for target_service, service_mods in service_rules.items():
                # Only apply to services in this compose file
                if target_service not in compose_yaml.get('services', {}):
                    continue
                
                svc = compose_yaml['services'][target_service]
                
                # Apply depends_on
                if 'depends_on' in service_mods:
                    existing = svc.get('depends_on', [])
                    if type(existing) == 'list':
                        for dep in service_mods['depends_on']:
                            if dep not in existing:
                                existing.append(dep)
                        svc['depends_on'] = existing
                    else:
                        # Dict format
                        for dep in service_mods['depends_on']:
                            if dep not in existing:
                                existing[dep] = {'condition': 'service_started'}
                        svc['depends_on'] = existing
                    modified = True
                
                # Apply volumes
                if 'volumes' in service_mods:
                    existing = svc.get('volumes', [])
                    for vol in service_mods['volumes']:
                        # Extract volume name for top-level declaration
                        vol_name = vol.split(':')[0] if ':' in vol else None
                        
                        # Check for duplicate mount points
                        mount_point = vol.split(':')[1] if ':' in vol else vol
                        already_mounted = False
                        for v in existing:
                            if mount_point in str(v):
                                already_mounted = True
                                break
                        if not already_mounted:
                            existing.append(vol)
                        
                        # Always ensure named volumes are in top-level volumes section
                        # Named volumes are in format "volume_name:/path" (not starting with / or .)
                        if vol_name and not vol_name.startswith('/') and not vol_name.startswith('.'):
                            if 'volumes' not in compose_yaml:
                                compose_yaml['volumes'] = {}
                            if vol_name not in compose_yaml['volumes']:
                                # Add as empty dict (Docker Compose default)
                                compose_yaml['volumes'][vol_name] = None
                    
                    svc['volumes'] = existing
                    modified = True
                
                # Apply environment
                if 'environment' in service_mods:
                    existing = svc.get('environment', {})
                    if type(existing) == 'list':
                        # Convert list format to dict
                        env_dict = {}
                        for e in existing:
                            if '=' in str(e):
                                parts = str(e).split('=', 1)
                                env_dict[parts[0]] = parts[1]
                        existing = env_dict
                    for k, v in service_mods['environment'].items():
                        existing[k] = v
                    svc['environment'] = existing
                    modified = True
                
                # Apply labels
                if 'labels' in service_mods:
                    existing = svc.get('labels', {})
                    if type(existing) == 'list':
                        # Convert list format to dict
                        labels_dict = {}
                        for label in existing:
                            if '=' in str(label):
                                parts = str(label).split('=', 1)
                                labels_dict[parts[0]] = parts[1]
                        existing = labels_dict
                    for k, v in service_mods['labels'].items():
                        existing[k] = v
                    svc['labels'] = existing
                    modified = True
                
                if modified:
                    print("    [wire_when] " + source + " wired " + target_service + " for " + trigger_dep)
    
    return compose_yaml

# ============================================================================
# Compose File Staging
# ============================================================================

def _stage_compose_file(dep_name, content, staging_dir):
    """
    Write modified compose content to staging directory.
    
    Returns the absolute path to the staged file.
    """
    staged_path = staging_dir + '/' + dep_name + '.yaml'
    
    # Ensure staging directory exists
    local('mkdir -p "' + staging_dir + '"', quiet=True)
    
    # Write the YAML content
    yaml_content = encode_yaml(content)
    local('cat > "' + staged_path + '"', stdin=yaml_content, quiet=True)
    
    return staged_path

def _generate_include_entry(entry):
    """
    Generate the appropriate include directive entry.
    
    - Unmodified: simple path string
    - Modified: object with path and project_directory
    """
    if not entry['modified']:
        return entry['compose_path']
    else:
        return {
            'path': entry['staged_path'],
            'project_directory': entry['project_directory'],
        }

# ============================================================================
# Main Entry Point
# ============================================================================

def generate_master_compose(
    dependencies,
    local_compose_paths=[],
    staging_dir=None,
):
    """
    Generate master compose file by assembling dependencies.
    
    This function is symmetric - any plugin can be the orchestrator and
    the result will be the same, because wiring rules are defined
    declaratively in extensions via get_wire_when().
    
    Args:
        dependencies: List of dependency dicts. Each must have:
            - name: str (required)
            - url: str (required)
            - compose_overrides: dict (optional) - static overrides
        local_compose_paths: Additional local compose file paths to include
        staging_dir: Directory for modified compose files.
                     Defaults to .compose-stage/ in current working directory.
    
    Returns:
        Dict with 'include' key for docker_compose()
    """
    staging_dir = staging_dir if staging_dir else os.path.abspath('.compose-stage')
    
    # ========================================================================
    # Phase 1: Load all extensions
    # ========================================================================
    print("\nLoading extensions:")
    
    loaded_deps = []
    loaded_dep_names = []
    loaded_symbols = {}
    
    for dep in dependencies:
        if 'url' not in dep:
            fail("Dependency " + dep['name'] + " must have 'url' field")
        
        print("  Loading: " + dep['name'])
        _load_extension(dep)
        
        loaded_deps.append(dep)
        loaded_dep_names.append(dep['name'])
        loaded_symbols[dep['name']] = dep.get('symbols', {})
    
    # ========================================================================
    # Phase 2: Collect wire_when rules from all extensions
    # ========================================================================
    print("\nCollecting wiring rules:")
    wire_when_rules = _collect_wire_when_rules(loaded_deps)
    
    for trigger, rule_sets in wire_when_rules.items():
        sources = [rs['source_dep'] for rs in rule_sets]
        print("  " + trigger + " triggers: " + ", ".join(sources))
    
    if not wire_when_rules:
        print("  (no wire_when rules defined)")
    
    # ========================================================================
    # Phase 3: Resolve and transform compose files
    # ========================================================================
    print("\nAssembling compose files:")
    resolved = []
    
    for dep in loaded_deps:
        compose_path = _get_compose_path_from_extension(dep)
        project_dir = os.path.dirname(compose_path)
        
        print("  " + dep['name'] + ": " + compose_path)
        
        # Read the compose file content
        content = read_yaml(compose_path)
        original_content = _deep_copy(content)
        
        # Apply static compose_overrides from dependency definition
        overrides = dep.get('compose_overrides')
        if overrides:
            print("    -> Applying compose_overrides")
            content = _deep_merge(content, overrides)
        
        # Apply wire_when rules
        content = _apply_wire_when_rules(content, dep['name'], wire_when_rules, loaded_dep_names)
        
        entry = {
            'dep': dep,
            'compose_path': compose_path,
            'project_directory': project_dir,
            'content': content,
            'original_content': original_content,
            'modified': False,
            'staged_path': None,
        }
        
        # Check if content was modified
        if encode_yaml(entry['content']) != encode_yaml(entry['original_content']):
            entry['modified'] = True
            entry['staged_path'] = _stage_compose_file(
                dep['name'],
                entry['content'],
                staging_dir,
            )
            print("    -> Modified, staged to: " + entry['staged_path'])
        
        resolved.append(entry)
    
    # ========================================================================
    # Phase 4: Generate include list
    # ========================================================================
    print("\nGenerated master compose:")
    includes = []
    
    for entry in resolved:
        include_entry = _generate_include_entry(entry)
        includes.append(include_entry)
        if entry['modified']:
            print("  - " + entry['dep']['name'] + ": " + entry['staged_path'] + " (project_dir: " + entry['project_directory'] + ")")
        else:
            print("  - " + entry['dep']['name'] + ": " + entry['compose_path'])
    
    # Add local compose paths
    for path in local_compose_paths:
        includes.append(path)
        print("  - local: " + path)
    
    return {'include': includes}
