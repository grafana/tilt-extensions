# Compose Composer Extension
# Dynamically assembles docker-compose files using the `include` directive
# to preserve relative path resolution for each component.
#
# Design Principle:
#   Any plugin can be the orchestrator (composer). The result should be the same
#   regardless of which plugin initiates the composition. This is achieved through:
#   - Declarative wiring rules in extensions (get_wire_when)
#   - Dependency graphs that can be merged (get_dependency_graph)
#   - compose_overrides that are deep-merged when dependencies overlap
#
# Usage:
#   load('ext://compose_composer', 'generate_master_compose', 'parse_cli_plugins')
#   
#   cli_plugins = parse_cli_plugins(os.path.dirname(__file__))
#   dependencies = core_dependencies + cli_plugins
#   master_compose = generate_master_compose(dependencies)
#   docker_compose(encode_yaml(master_compose))
#
# CLI Usage:
#   tilt up -- plugin-name ../relative/path /absolute/path https://github.com/org/repo.git
#
# Dependency schema:
#   name: str                      - Extension name (required)
#   url: str                       - Extension repo URL, file:// or https:// (required)
#   ref: str                       - Git ref for https:// URLs (default: 'main')
#   repo_path: str                 - Path within repo (default: name)
#   get_compose_path_function: str - Function to call for compose path (default: 'get_compose_path')
#   compose_overrides: dict        - Static overrides merged into compose (optional)
#
# Extension exports:
#   get_compose_path() -> str         - Returns path to compose file (required)
#   get_wire_when() -> dict           - Conditional wiring rules (optional)
#   get_provides() -> dict            - What this extension provides (optional, for docs)
#   get_dependency_graph() -> dict    - Dependencies this extension brings (optional)
#                                       Returns: {dependencies: [...], compose_path: str}

# ============================================================================
# CLI Argument Parsing
# ============================================================================

config.define_string_list('args', args=True, usage='Plugins to load (name, path, or URL)')
_cfg = config.parse()

def _is_url(s):
    """Check if string is a URL (git or https)."""
    return '://' in s or s.startswith('git@')

def _resolve_plugin_spec(plugin, tiltfile_dir):
    """
    Resolve a plugin specifier to a dependency dict with url.
    - URLs pass through as https
    - Absolute paths become file:// URLs
    - Relative paths resolve from tiltfile_dir and become file:// URLs
    - Plain names are treated as adjacent directories
    """
    # Git/HTTPS URLs
    if _is_url(plugin):
        # Extract name from URL (last segment, minus extensions)
        name = plugin.split('/')[-1].split('#')[0].replace('.git', '')
        return {'name': name, 'url': plugin, '_from_cli': True}
    
    # Resolve local paths to absolute
    if plugin.startswith('/'):
        path = plugin
    elif plugin.startswith('.'):
        path = os.path.abspath(tiltfile_dir + '/' + plugin)
    else:
        # Adjacent directory - look in parent
        path = os.path.abspath(tiltfile_dir + '/../' + plugin)
    
    # Derive name from path
    name = os.path.basename(path)
    
    # Convert to file:// URL for extension loading
    # For file:// URLs, the path IS the repo, so repo_path should be '.'
    return {
        'name': name,
        'url': 'file://' + path,
        'repo_path': '.',
        '_from_cli': True,
    }

def parse_cli_plugins(tiltfile_dir):
    """
    Parse CLI positional args and return list of dependency dicts.
    
    Args:
        tiltfile_dir: Directory of the calling Tiltfile (for relative path resolution)
                      Typically: os.path.dirname(__file__)
    
    Returns:
        List of dependency dicts with 'name' and 'url' keys
    """
    plugins = _cfg.get('args', [])
    deps = []
    
    for plugin in plugins:
        dep = _resolve_plugin_spec(plugin, tiltfile_dir)
        deps.append(dep)
        print("CLI plugin: " + dep['name'] + " -> " + dep['url'])
    
    return deps

# ============================================================================
# Dependency Graph Expansion and Merging
# ============================================================================

def _expand_dependency_graphs(dependencies, loaded_extensions):
    """
    Expand dependencies by loading CLI plugins' dependency graphs.
    
    When a CLI plugin exports get_dependency_graph(), its dependencies are
    merged into the main list. This allows plugins to bring their own
    dependency trees.
    
    Args:
        dependencies: Initial list of dependencies
        loaded_extensions: Dict tracking loaded extensions (mutable)
    
    Returns:
        Expanded list with merged dependencies (deduplicated by name)
    """
    expanded = []
    seen_names = {}
    
    # First pass: collect all dependencies including those from CLI plugin graphs
    all_deps = []
    
    for dep in dependencies:
        all_deps.append(dep)
        
        # If this is a CLI plugin, we need to load it first to check for get_dependency_graph
        if dep.get('_from_cli'):
            # Temporarily load the extension to check for dependency graph
            _load_extension(dep, loaded_extensions)
            symbols = dep.get('symbols', {})
            get_dep_graph = symbols.get('get_dependency_graph')
            
            if get_dep_graph:
                graph = get_dep_graph()
                graph_deps = graph.get('dependencies', [])
                
                print("  [graph] " + dep['name'] + " brings " + str(len(graph_deps)) + " dependencies")
                
                for sub_dep in graph_deps:
                    # Mark the source so we can trace where it came from
                    sub_dep['_from_graph'] = dep['name']
                    all_deps.append(sub_dep)
    
    # Second pass: deduplicate by name, merging compose_overrides
    for dep in all_deps:
        name = dep['name']
        
        if name in seen_names:
            # Merge with existing
            existing = seen_names[name]
            existing = _merge_dependency_configs(existing, dep)
            seen_names[name] = existing
            
            # Update in expanded list
            for i, e in enumerate(expanded):
                if e['name'] == name:
                    expanded[i] = existing
                    break
        else:
            seen_names[name] = dep
            expanded.append(dep)
    
    return expanded

def _merge_dependency_configs(existing, new):
    """
    Merge two dependency configs with the same name.
    
    - compose_overrides are deep-merged
    - Other fields from 'new' override 'existing' (except internal fields)
    """
    result = {}
    
    # Copy existing fields
    for key in existing:
        result[key] = existing[key]
    
    # Merge/override with new fields (except internal tracking fields)
    for key in new:
        if key.startswith('_'):
            continue  # Skip internal fields like _from_cli, _from_graph
        
        if key == 'compose_overrides':
            # Deep merge compose_overrides
            existing_overrides = result.get('compose_overrides', {})
            new_overrides = new.get('compose_overrides', {})
            result['compose_overrides'] = _deep_merge(existing_overrides, new_overrides)
        elif key not in result:
            result[key] = new[key]
    
    return result

# ============================================================================
# Extension Loading
# ============================================================================

def _load_extension(dep, loaded_extensions):
    """Load a Tiltfile extension and populate its symbols."""
    name = dep['name']
    
    # Check if already loaded (either by us or by the orchestrator loading helpers)
    if name in loaded_extensions:
        dep['symbols'] = loaded_extensions[name]
        return dep
    
    # Check if symbols already exist (loaded by orchestrator before calling generate_master_compose)
    if dep.get('symbols'):
        loaded_extensions[name] = dep['symbols']
        return dep
    
    url = dep['url']
    ref = dep.get('ref', 'main')
    repo_path = dep.get('repo_path', name)
    
    # Use a unique repo name to avoid conflicts with orchestrator's registrations
    repo_name = 'cc-' + name
    
    # Register and load
    if url.startswith('file://'):
        v1alpha1.extension_repo(name=repo_name, url=url)
    else:
        v1alpha1.extension_repo(name=repo_name, url=url, ref=ref)
    
    v1alpha1.extension(name=name, repo_name=repo_name, repo_path=repo_path)
    dep['symbols'] = load_dynamic('ext://' + name)
    loaded_extensions[name] = dep['symbols']
    return dep

def _get_compose_path_from_extension(dep):
    """Get compose path from a loaded extension's function."""
    fn_name = dep.get('get_compose_path_function', 'get_compose_path')
    symbols = dep.get('symbols')
    
    if not symbols:
        fail("Extension " + dep['name'] + " has no symbols")
    
    fn = symbols.get(fn_name)
    if not fn:
        fail("Extension " + dep['name'] + " has no " + fn_name + " function")
    
    return fn()

# ============================================================================
# Deep Merge Utility
# ============================================================================

def _deep_merge(base, override):
    """
    Deep merge override into base.
    - Dicts are merged recursively
    - Lists are concatenated (for volumes, depends_on, etc.)
    - Other values are replaced
    
    Returns a new dict (does not modify base).
    """
    if type(base) != 'dict' or type(override) != 'dict':
        return override
    
    result = {}
    
    # Start with all keys from base
    for key in base:
        result[key] = base[key]
    
    # Merge/override with keys from override
    for key in override:
        if key in result:
            if type(result[key]) == 'dict' and type(override[key]) == 'dict':
                result[key] = _deep_merge(result[key], override[key])
            elif type(result[key]) == 'list' and type(override[key]) == 'list':
                # Concatenate lists, avoiding duplicates for simple values
                merged = list(result[key])
                for item in override[key]:
                    if item not in merged:
                        merged.append(item)
                result[key] = merged
            else:
                result[key] = override[key]
        else:
            result[key] = override[key]
    
    return result

def _deep_copy(obj):
    """Create a deep copy of a dict/list structure using YAML round-trip."""
    return decode_yaml(encode_yaml(obj))

# ============================================================================
# Declarative Wiring (wire_when)
# ============================================================================

def _collect_wire_when_rules(loaded_deps):
    """
    Collect wire_when rules from all loaded extensions.
    
    Returns a dict: {trigger_dep_name: [{source_dep, rules}, ...]}
    """
    all_rules = {}
    
    for dep in loaded_deps:
        symbols = dep.get('symbols', {})
        get_wire_when = symbols.get('get_wire_when')
        
        if not get_wire_when:
            continue
        
        wire_when = get_wire_when()
        
        for trigger_dep_name, rules in wire_when.items():
            if trigger_dep_name not in all_rules:
                all_rules[trigger_dep_name] = []
            
            all_rules[trigger_dep_name].append({
                'source_dep': dep['name'],
                'rules': rules,
            })
    
    return all_rules

def _apply_wire_when_rules(compose_yaml, dep_name, wire_when_rules, loaded_dep_names):
    """
    Apply wire_when rules to a compose file.
    
    Args:
        compose_yaml: The compose content to modify
        dep_name: Name of the dependency being processed
        wire_when_rules: All collected wire_when rules
        loaded_dep_names: List of all loaded dependency names
    
    Returns:
        Modified compose_yaml
    """
    modified = False
    
    # Check each trigger dependency
    for trigger_dep, rule_sets in wire_when_rules.items():
        # Only apply if the trigger dependency is loaded
        if trigger_dep not in loaded_dep_names:
            continue
        
        # Apply each rule set from extensions that defined rules for this trigger
        for rule_set in rule_sets:
            rules = rule_set['rules']
            source = rule_set['source_dep']
            
            # Get service rules
            service_rules = rules.get('services', {})
            
            for target_service, service_mods in service_rules.items():
                # Only apply to services in this compose file
                if target_service not in compose_yaml.get('services', {}):
                    continue
                
                svc = compose_yaml['services'][target_service]
                
                # Apply depends_on
                if 'depends_on' in service_mods:
                    existing = svc.get('depends_on', [])
                    if type(existing) == 'list':
                        for dep in service_mods['depends_on']:
                            if dep not in existing:
                                existing.append(dep)
                        svc['depends_on'] = existing
                    else:
                        # Dict format
                        for dep in service_mods['depends_on']:
                            if dep not in existing:
                                existing[dep] = {'condition': 'service_started'}
                        svc['depends_on'] = existing
                    modified = True
                
                # Apply volumes
                if 'volumes' in service_mods:
                    existing = svc.get('volumes', [])
                    for vol in service_mods['volumes']:
                        # Extract volume name for top-level declaration
                        vol_name = vol.split(':')[0] if ':' in vol else None
                        
                        # Check for duplicate mount points
                        mount_point = vol.split(':')[1] if ':' in vol else vol
                        already_mounted = False
                        for v in existing:
                            if mount_point in str(v):
                                already_mounted = True
                                break
                        if not already_mounted:
                            existing.append(vol)
                        
                        # Always ensure named volumes are in top-level volumes section
                        # Named volumes are in format "volume_name:/path" (not starting with / or .)
                        if vol_name and not vol_name.startswith('/') and not vol_name.startswith('.'):
                            if 'volumes' not in compose_yaml:
                                compose_yaml['volumes'] = {}
                            if vol_name not in compose_yaml['volumes']:
                                # Add as empty dict (Docker Compose default)
                                compose_yaml['volumes'][vol_name] = None
                    
                    svc['volumes'] = existing
                    modified = True
                
                # Apply environment
                if 'environment' in service_mods:
                    existing = svc.get('environment', {})
                    if type(existing) == 'list':
                        # Convert list format to dict
                        env_dict = {}
                        for e in existing:
                            if '=' in str(e):
                                parts = str(e).split('=', 1)
                                env_dict[parts[0]] = parts[1]
                        existing = env_dict
                    for k, v in service_mods['environment'].items():
                        existing[k] = v
                    svc['environment'] = existing
                    modified = True
                
                # Apply labels
                if 'labels' in service_mods:
                    existing = svc.get('labels', {})
                    if type(existing) == 'list':
                        # Convert list format to dict
                        labels_dict = {}
                        for label in existing:
                            if '=' in str(label):
                                parts = str(label).split('=', 1)
                                labels_dict[parts[0]] = parts[1]
                        existing = labels_dict
                    for k, v in service_mods['labels'].items():
                        existing[k] = v
                    svc['labels'] = existing
                    modified = True
                
                if modified:
                    print("    [wire_when] " + source + " wired " + target_service + " for " + trigger_dep)
    
    return compose_yaml

# ============================================================================
# Compose File Staging
# ============================================================================

def _stage_compose_file(dep_name, content, staging_dir):
    """
    Write modified compose content to staging directory.
    
    Returns the absolute path to the staged file.
    """
    staged_path = staging_dir + '/' + dep_name + '.yaml'
    
    # Ensure staging directory exists
    local('mkdir -p "' + staging_dir + '"', quiet=True)
    
    # Write the YAML content
    yaml_content = encode_yaml(content)
    local('cat > "' + staged_path + '"', stdin=yaml_content, quiet=True)
    
    return staged_path

def _generate_include_entry(entry):
    """
    Generate the appropriate include directive entry.
    
    - Unmodified: simple path string
    - Modified: object with path and project_directory
    """
    if not entry['modified']:
        return entry['compose_path']
    else:
        return {
            'path': entry['staged_path'],
            'project_directory': entry['project_directory'],
        }

# ============================================================================
# Main Entry Point
# ============================================================================

def generate_master_compose(
    dependencies,
    local_compose_paths=[],
    staging_dir=None,
):
    """
    Generate master compose file by assembling dependencies.
    
    This function is symmetric - any plugin can be the orchestrator and
    the result will be the same, because:
    - Wiring rules are defined declaratively in extensions (get_wire_when)
    - Dependency graphs are expanded and merged (get_dependency_graph)
    - compose_overrides are deep-merged when dependencies overlap
    
    Args:
        dependencies: List of dependency dicts. Each must have:
            - name: str (required)
            - url: str (required)
            - compose_overrides: dict (optional) - static overrides
        local_compose_paths: Additional local compose file paths to include
        staging_dir: Directory for modified compose files.
                     Defaults to .compose-stage/ in current working directory.
    
    Returns:
        Dict with 'include' key for docker_compose()
    """
    staging_dir = staging_dir if staging_dir else os.path.abspath('.compose-stage')
    
    # Track loaded extensions to avoid re-registration
    # This dict is passed through all phases and mutated as extensions are loaded
    loaded_extensions = {}
    
    # Pre-populate with any extensions that were already loaded by the orchestrator
    for dep in dependencies:
        if dep.get('symbols'):
            loaded_extensions[dep['name']] = dep['symbols']
    
    # ========================================================================
    # Phase 0: Expand dependency graphs from CLI plugins
    # ========================================================================
    # CLI plugins may export get_dependency_graph() which brings their own
    # dependencies. These are merged with the orchestrator's dependencies,
    # deduplicating by name and merging compose_overrides.
    print("\nExpanding dependency graphs:")
    dependencies = _expand_dependency_graphs(dependencies, loaded_extensions)
    print("  Total dependencies after expansion: " + str(len(dependencies)))
    for dep in dependencies:
        source = ""
        if dep.get('_from_cli'):
            source = " (CLI)"
        elif dep.get('_from_graph'):
            source = " (from " + dep['_from_graph'] + ")"
        print("    - " + dep['name'] + source)
    
    # ========================================================================
    # Phase 1: Load all extensions
    # ========================================================================
    print("\nLoading extensions:")
    
    loaded_deps = []
    loaded_dep_names = []
    loaded_symbols = {}
    
    for dep in dependencies:
        if 'url' not in dep:
            fail("Dependency " + dep['name'] + " must have 'url' field")
        
        # Skip if already loaded during graph expansion or by orchestrator
        if dep.get('symbols'):
            print("  Already loaded: " + dep['name'])
        else:
            print("  Loading: " + dep['name'])
            _load_extension(dep, loaded_extensions)
        
        loaded_deps.append(dep)
        loaded_dep_names.append(dep['name'])
        loaded_symbols[dep['name']] = dep.get('symbols', {})
    
    # ========================================================================
    # Phase 2: Collect wire_when rules from all extensions
    # ========================================================================
    print("\nCollecting wiring rules:")
    wire_when_rules = _collect_wire_when_rules(loaded_deps)
    
    for trigger, rule_sets in wire_when_rules.items():
        sources = [rs['source_dep'] for rs in rule_sets]
        print("  " + trigger + " triggers: " + ", ".join(sources))
    
    if not wire_when_rules:
        print("  (no wire_when rules defined)")
    
    # ========================================================================
    # Phase 3: Resolve and transform compose files
    # ========================================================================
    print("\nAssembling compose files:")
    resolved = []
    
    for dep in loaded_deps:
        compose_path = _get_compose_path_from_extension(dep)
        project_dir = os.path.dirname(compose_path)
        
        print("  " + dep['name'] + ": " + compose_path)
        
        # Read the compose file content
        content = read_yaml(compose_path)
        original_content = _deep_copy(content)
        
        # Apply static compose_overrides from dependency definition
        # Note: CRD mounts come through compose_overrides via register_crds() helper
        overrides = dep.get('compose_overrides')
        if overrides:
            print("    -> Applying compose_overrides")
            content = _deep_merge(content, overrides)
        
        # Apply wire_when rules
        content = _apply_wire_when_rules(content, dep['name'], wire_when_rules, loaded_dep_names)
        
        entry = {
            'dep': dep,
            'compose_path': compose_path,
            'project_directory': project_dir,
            'content': content,
            'original_content': original_content,
            'modified': False,
            'staged_path': None,
        }
        
        # Check if content was modified
        if encode_yaml(entry['content']) != encode_yaml(entry['original_content']):
            entry['modified'] = True
            entry['staged_path'] = _stage_compose_file(
                dep['name'],
                entry['content'],
                staging_dir,
            )
            print("    -> Modified, staged to: " + entry['staged_path'])
        
        resolved.append(entry)
    
    # ========================================================================
    # Phase 4: Generate include list
    # ========================================================================
    print("\nGenerated master compose:")
    includes = []
    
    for entry in resolved:
        include_entry = _generate_include_entry(entry)
        includes.append(include_entry)
        if entry['modified']:
            print("  - " + entry['dep']['name'] + ": " + entry['staged_path'] + " (project_dir: " + entry['project_directory'] + ")")
        else:
            print("  - " + entry['dep']['name'] + ": " + entry['compose_path'])
    
    # Add local compose paths
    for path in local_compose_paths:
        includes.append(path)
        print("  - local: " + path)
    
    return {'include': includes}

# ============================================================================
# Test Exports
# ============================================================================
# These are exported for unit testing. Not intended for general use.

def test_exports():
    """Returns internal functions for testing."""
    return {
        'deep_merge': _deep_merge,
        'deep_copy': _deep_copy,
        'merge_dependency_configs': _merge_dependency_configs,
        'collect_wire_when_rules': _collect_wire_when_rules,
        'apply_wire_when_rules': _apply_wire_when_rules,
        'is_url': _is_url,
        'resolve_plugin_spec': _resolve_plugin_spec,
    }
